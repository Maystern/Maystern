
## Self Introduction
I am **Jiacheng Luo (罗嘉诚)**, a junior student majoring in **[CSE](https://cse.sustech.edu.cn/en/)** , specializing in **Computer Science and Technology** at  **[SUSTech](https://www.sustech.edu.cn/en/)**. Currently, my **academic advisor** is **[Prof. Jianguo Zhang](https://www.sustech.edu.cn/en/faculties/zhangjianguo.html)**, and **my life advisor** is **[Assistant Prof. Bin Zhu](https://www.sustech.edu.cn/en/faculties/bin-zhu.html)**. 

-  **[Prof. Jianguo Zhang](https://www.sustech.edu.cn/en/faculties/zhangjianguo.html)** is the leader of the **[CVIP Group](https://faculty.sustech.edu.cn/?tagid=zhangjg&iscss=1&snapid=1&orderby=date&go=1&lang=en)** laboratory at SUSTech and has previously served as a Reader in the School of Science and Engineering at the University of Dundee, UK, as well as the Director of International Cooperation in the Department of Computer Science. 
- **[Dr. Bin Zhu](https://www.sustech.edu.cn/en/faculties/bin-zhu.html)** is an assistant professor and doctoral supervisor of the **[SPHEM](https://sph.sustech.edu.cn/)** (School of Public Health and Emergency Management) in SUSTech.

The main **research areas** of the **CVIP Group** laboratory are ***computer vision***, ***medical image and information processing***, ***machine learning***, and ***artificial intelligence***. 

My research interests include ***Domain Adaptation***, ***Transfer Learning***, ***Parameter-Efficient Fine-Tuning (PEFT)*** and ***Large Model Training***. 

This is my personal academic homepage: [https://maystern.github.io](https://maystern.github.io). Welcome to visit!

***Contact Me***
- email ✉️ luojc2021@mail.sustech.edu.cn / mr.jiachengluo@gmail.com

## Academic Background

- ***Sept 2021 - Jun 2025***: Southern University of Science and Technology (BEng.)

## Research Interests


<html>
<body>
    <details>
        <summary><strong><em>Multi-Modal Machine Learning (MMML)</em></strong></summary>
        <div style="margin-left: 20px;"> <!-- 调整这里的数值以控制缩进量 -->
            &nbsp; &nbsp; Humans perceive the world through various sensory organs, such as the eyes, ears, and tactile senses. Multi-Modal Machine Learning (MMML) research addresses machine learning problems with different modalities of data. Common modalities include vision, text, and sound. They usually come from different sensors, and the formation of data and internal structure differ significantly. For example, images are a continuous space that naturally exists in the world, while text is a discrete space organized by human knowledge and grammar rules. The heterogeneity of multimodal data poses challenges for learning the correlations and complementarities among them.
        </div>
    </details>
</body>
</html>

<html>
<body>
    <details>
        <summary><strong><em>Few-Shot Learning (FSL)</em></strong></summary>
        <div style="margin-left: 20px;"> <!-- 调整这里的数值以控制缩进量 -->
          &nbsp; &nbsp; Few-shot learning (FSL) is a machine learning method that trains with limited information datasets. The common practice in machine learning application fields is to provide models with as much data as possible. This is because in most machine learning applications, providing more data helps the model in making better predictions. However, few-shot learning aims to construct accurate machine learning models with fewer training data. Since the dimensionality of input data determines the cost of resources (such as time cost, computational cost, etc.), people can lower the cost of data analysis/machine learning (ML) by using few-shot learning.
        </div>
    </details>
</body>
</html>

<html>
<body>
    <details>
        <summary><strong><em>Parameter-Efficient Fine-Tuning (PEFT)</em></strong></summary>
        <div style="margin-left: 20px;"> <!-- 调整这里的数值以控制缩进量 -->
          &nbsp; &nbsp; In recent years, there have been many large pre-trained models in deep learning research, such as GPT-3, BERT, ViT, etc., which can achieve excellent performance in various natural language and even visual image processing tasks. However, the training cost of these large pre-trained models is very high, requiring a huge amount of computational resources and data. The Parameter-Efficient Fine-Tuning (PEFT) technique aims to improve the performance of pre-trained models on new tasks by minimizing the number of fine-tuning parameters and computational complexity, thus easing the training cost of large pre-trained models and achieving efficient transfer learning.
        </div>
    </details>
</body>
</html>


## News and Updates

- ***`[Aug 31,2023]`*** My personal academic website is online!
- ***`[Jul 18,2023]`*** Honored to join the CVIP Group as a formal member and hope to do a good job!
- ***`[Aug 22,2022]`*** Happy to join the CVIP Group as an unofficial attending student!
