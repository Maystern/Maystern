
## Self Introduction
I am **Jiacheng Luo (罗嘉诚)**, a junior student majoring in **[CSE](https://cse.sustech.edu.cn/en/)** , specializing in **Computer Science and Technology** at  **[SUSTech](https://www.sustech.edu.cn/en/)**. Currently, my **academic advisor** is **[Prof. Jianguo Zhang](https://www.sustech.edu.cn/en/faculties/zhangjianguo.html)**, and **my life advisor** is **[Assistant Prof. Bin Zhu](https://www.sustech.edu.cn/en/faculties/bin-zhu.html)**. 

-  **[Prof. Jianguo Zhang](https://www.sustech.edu.cn/en/faculties/zhangjianguo.html)** is the leader of the **[CVIP Group](https://faculty.sustech.edu.cn/?tagid=zhangjg&iscss=1&snapid=1&orderby=date&go=1&lang=en)** laboratory at SUSTech and has previously served as a Reader in the School of Science and Engineering at the University of Dundee, UK, as well as the Director of International Cooperation in the Department of Computer Science. 
- **[Dr. Bin Zhu](https://www.sustech.edu.cn/en/faculties/bin-zhu.html)** is an assistant professor and doctoral supervisor of the **[SPHEM](https://sph.sustech.edu.cn/)** (School of Public Health and Emergency Management) in SUSTech.

The main **research areas** of the **CVIP Group** laboratory are ***computer vision***, ***medical image and information processing***, ***machine learning***, and ***artificial intelligence***. 

My research interests include ***Domain Adaptation***, ***Transfer Learning***, ***Parameter-Efficient Fine-Tuning (PEFT)*** and ***Large Model Training***. 

This is my personal academic homepage: [https://maystern.github.io](https://maystern.github.io). Welcome to visit!

***Contact Me***
- email ✉️ luojc2021@mail.sustech.edu.cn / mr.jiachengluo@gmail.com

## Academic Background

- ***Sept 2021 - Jun 2025***: Southern University of Science and Technology (BEng.)

## Research Interests


<html>
<body>
    <details>
        <summary><strong><em>Domain Adaptation</em></strong></summary>
        <div style="margin-left: 20px;"> <!-- 调整这里的数值以控制缩进量 -->
            &nbsp; &nbsp; Domain adaptation refers to the process of adapting a machine learning model trained on data from one domain to perform effectively on data from a different but related domain. It addresses the challenge of domain shift, where the distributions of data between the source and target domains may differ. The goal is to minimize this distribution discrepancy and improve the model's performance on the target domain without requiring labeled data from the target domain.
        </div>
    </details>
</body>
</html>

<html>
<body>
    <details>
        <summary><strong><em>Transfer Learning</em></strong></summary>
        <div style="margin-left: 20px;"> <!-- 调整这里的数值以控制缩进量 -->
          &nbsp; &nbsp; Transfer learning is a technique in machine learning where knowledge gained from solving one problem is applied to a different but related problem. By leveraging pre-trained models or learned representations from a source domain, transfer learning allows for improved performance on a target task, particularly when labeled data in the target domain is limited. This approach has shown significant success across various domains, including computer vision, natural language processing, and speech recognition, enabling faster model training and better generalization with reduced data and computational resources.
        </div>
    </details>
</body>
</html>

<html>
<body>
    <details>
        <summary><strong><em>Parameter-Efficient Fine-Tuning (PEFT)</em></strong></summary>
        <div style="margin-left: 20px;"> <!-- 调整这里的数值以控制缩进量 -->
          &nbsp; &nbsp; In recent years, there have been many large pre-trained models in deep learning research, such as GPT-3, BERT, ViT, etc., which can achieve excellent performance in various natural language and even visual image processing tasks. However, the training cost of these large pre-trained models is very high, requiring a huge amount of computational resources and data. The Parameter-Efficient Fine-Tuning (PEFT) technique aims to improve the performance of pre-trained models on new tasks by minimizing the number of fine-tuning parameters and computational complexity, thus easing the training cost of large pre-trained models and achieving efficient transfer learning.
        </div>
    </details>
</body>
</html>

<html>
<body>
    <details>
        <summary><strong><em>Large Model Training</em></strong></summary>
        <div style="margin-left: 20px;"> <!-- 调整这里的数值以控制缩进量 -->
          &nbsp; &nbsp; 
Large model training involves the process of training machine learning or deep learning models that possess a significant number of parameters or exhibit complex architectures. It necessitates substantial computational resources, such as GPUs or TPUs, along with extensive datasets for effective training. Employing optimization algorithms like stochastic gradient descent (SGD) or its variants, large model training iteratively fine-tunes model parameters to optimize performance. Techniques like mini-batch training, regularization, and learning rate scheduling are often employed to enhance convergence and mitigate overfitting. This approach finds widespread application in fields like natural language processing, computer vision, and reinforcement learning, where intricate data patterns require sophisticated models for effective analysis and prediction.
        </div>
    </details>
</body>
</html>


## News and Updates

- ***`[Aug 31,2023]`*** My personal academic website is online!
- ***`[Jul 18,2023]`*** Honored to join the CVIP Group as a formal member and hope to do a good job!
- ***`[Aug 22,2022]`*** Happy to join the CVIP Group as an unofficial attending student!
