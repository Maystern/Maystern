[[homepage](https://maystern.github.io)] [[CV](./docs/resume_jiacheng.pdf)]
## Self Introduction
I am **Jiacheng Luo (罗嘉诚)**, a junior student majoring in **[CSE](https://cse.sustech.edu.cn/en/)** , specializing in **Computer Science and Technology** at  **[SUSTech](https://www.sustech.edu.cn/en/)**. Currently, my **academic advisor** is **[Prof. Jianguo Zhang](https://www.sustech.edu.cn/en/faculties/zhangjianguo.html)**, and **my life advisor** is **[Assistant Prof. Bin Zhu](https://www.sustech.edu.cn/en/faculties/bin-zhu.html)**. 

-  **[Prof. Jianguo Zhang](https://www.sustech.edu.cn/en/faculties/zhangjianguo.html)** is the leader of the **[CVIP Group](https://faculty.sustech.edu.cn/?tagid=zhangjg&iscss=1&snapid=1&orderby=date&go=1&lang=en)** laboratory at SUSTech and has previously served as a Reader in the School of Science and Engineering at the University of Dundee, UK, as well as the Director of International Cooperation in the Department of Computer Science. 
- **[Dr. Bin Zhu](https://www.sustech.edu.cn/en/faculties/bin-zhu.html)** is an assistant professor and doctoral supervisor of the **[SPHEM](https://sph.sustech.edu.cn/)** (School of Public Health and Emergency Management) in SUSTech.

The main **research areas** of the **CVIP Group** laboratory are ***computer vision***, ***medical image and information processing***, ***machine learning***, and ***artificial intelligence***. 

My research interests include ***Domain Adaptation***, ***Transfer Learning***, ***Parameter-Efficient Fine-Tuning*** and ***Large Model Training***. 

***Contact Me***
- email ✉️ luojc2021@mail.sustech.edu.cn / mr.jiachengluo@gmail.com

## Academic Background

- ***Sept 2021 - Jun 2025 (expected)***: Southern University of Science and Technology (BEng.)

## Research Interests


<html>
<body>
    <details>
        <summary><strong><em>Domain Adaptation</em></strong></summary>
        <div style="margin-left: 20px;"> <!-- 调整这里的数值以控制缩进量 -->
            &nbsp; &nbsp; Domain adaptation is a field associated with machine learning and transfer learning. This scenario arises when we aim at learning a model from a source data distribution and applying that model on a different (but related) target data distribution. For instance, one of the tasks of the common spam filtering problem consists in adapting a model from one user (the source distribution) to a new user who receives significantly different emails (the target distribution). Domain adaptation has also been shown to be beneficial for learning unrelated sources. Note that, when more than one source distribution is available the problem is referred to as multi-source domain adaptation.
        </div>
    </details>
</body>
</html>

<html>
<body>
    <details>
        <summary><strong><em>Transfer Learning</em></strong></summary>
        <div style="margin-left: 20px;"> <!-- 调整这里的数值以控制缩进量 -->
          &nbsp; &nbsp; Transfer learning is a technique in machine learning in which knowledge learned from a task is re-used in order to boost performance on a related task. For example, for image classification, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks. This topic is related to the psychological literature on transfer of learning, although practical ties between the two fields are limited. Reusing/transferring information from previously learned tasks to new tasks has the potential to significantly improve learning efficiency.
        </div>
    </details>
</body>
</html>

<html>
<body>
    <details>
        <summary><strong><em>Parameter-Efficient Fine-Tuning</em></strong></summary>
        <div style="margin-left: 20px;"> <!-- 调整这里的数值以控制缩进量 -->
          &nbsp; &nbsp; Parameter-efficient Fine-tuning (PEFT) is a technique used in Natural Language Processing (NLP) to improve the performance of pre-trained language models on specific downstream tasks. It involves reusing the pre-trained model’s parameters and fine-tuning them on a smaller dataset, which saves computational resources and time compared to training the entire model from scratch. PEFT achieves this efficiency by freezing some of the layers of the pre-trained model and only fine-tuning the last few layers that are specific to the downstream task. This way, the model can be adapted to new tasks with less computational overhead and fewer labeled examples. Although PEFT has been a relatively novel concept, updating the last layer of models has been in practice in the field of computer vision since the introduction of transfer learning. Even in NLP, experiments with static and non-static word embeddings were carried out early on. Parameter-efficient fine-tuning aims to improve the performance of pre-trained models, such as BERT and RoBERTa, on various downstream tasks, including sentiment analysis, named entity recognition, and question-answering. It achieves this in low-resource settings with limited data and computational resources. It modifies only a small subset of model parameters and is less prone to overfitting.
        </div>
    </details>
</body>
</html>

<html>
<body>
    <details>
        <summary><strong><em>Large Model Training</em></strong></summary>
        <div style="margin-left: 20px;"> <!-- 调整这里的数值以控制缩进量 -->
          &nbsp; &nbsp; 
Large model training involves the process of training machine learning or deep learning models that possess a significant number of parameters or exhibit complex architectures. It necessitates substantial computational resources, such as GPUs or TPUs, along with extensive datasets for effective training. Employing optimization algorithms like stochastic gradient descent (SGD) or its variants, large model training iteratively fine-tunes model parameters to optimize performance. Techniques like mini-batch training, regularization, and learning rate scheduling are often employed to enhance convergence and mitigate overfitting. This approach finds widespread application in fields like natural language processing, computer vision, and reinforcement learning, where intricate data patterns require sophisticated models for effective analysis and prediction.
        </div>
    </details>
</body>
</html>


## News and Updates
- ***`[Feb 02,2024]`*** One co-authored paper has been submitted to ICML 2024 for consideration.
- ***`[Aug 31,2023]`*** My personal academic website is online.
- ***`[Jul 18,2023]`*** Honored to join the CVIP Group as a formal member and hope to do a good job!
- ***`[Aug 22,2022]`*** Happy to join the CVIP Group as an unofficial attending student!
